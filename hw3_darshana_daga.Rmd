
title: "hw3_Darshana_Daga"
author: "Darshana"
date: "4/28/2021"

#### Question1
#### Reading data in and prepping the data
```{r}
dat <- read.csv("hw3_vio_crime_updated.csv")
head(dat)
str(dat)
dat1 <- dat[ -c(1:3) ] #removing the first 3 columns of the data
head(dat1)
```

#### Descriptive statastics 
```{r}
hist(dat1$VioCrime, main = "Crime rate (in $1000s)",
     xlab = "Violent Crimes", ylab = "Counts")
boxplot(dat1$VioCrime, main = "Crime rate (in $1000s)",
        horizontal = T, xlab = "Violent Crimes")
```


#### Question 2
```{r}
options(scipen=999)
2^100
```

As we can see that there are about 100 variables and it can have a very large
subset of about  1267650600228229401522626226666 which is computationally
very expensive and will take a lot of time to run through. Hence, its not feasible 
to perform best subset regression to pick a good model.

#### Question 3
```{r}
library(leaps)
reg3 <- regsubsets(VioCrime~.,data=dat1, nvmax = 100,
                         method = "forward", really.big = TRUE)

reg.summary <- summary(reg3)

#
#  Plot the diagnostic statistics computed by regsubsets
#  Plot adjusted R-squared, Mallows Cp and the Bayesian 
#    Information Criterion (BIC)
#
par(mfrow=c(2,2))
plot(reg.summary$rsq, xlab = "Number of Variables", ylab ="R-squared")
plot(reg.summary$adjr2, xlab = "Number of Variables", 
     ylab ="Adj R-squared")
which.max(reg.summary$adjr2)
maxar2 <- which.max(reg.summary$adjr2)
points(maxar2,reg.summary$adjr2[maxar2], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab ="Mallows Cp")
which.min(reg.summary$cp)
mincp <- which.min(reg.summary$cp)
points(mincp,reg.summary$cp[mincp], col = "blue", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables", 
     ylab ="Bayesian Info Crit")
which.min(reg.summary$bic)
minbic <- which.min(reg.summary$bic)
points(minbic,reg.summary$bic[minbic], col = "green", cex = 2, pch = 20)
q3 <- data.frame(maxar2,mincp,minbic)
q3
```
After running the stepwise forward regression & evaluating the outcomes from
Adjusted R-squared, Mallows Cp and BIC, it hard to make a judgment as all of them
are pointing towards different sets of variable (Max AdjustedR:64, CP: 51,& BIC:15)
Ideally the subset variable should make sense practically and have business 
implication. Machine alone cannot make decision, we need to understand the 
goal for the analysis and use the outcomes from machine as guidance. In this case,
further analysis will be required to understand which variables are providing
best business solution of correctly finding Violent crime numbers.


#### Question 4
```{r}
library(leaps)
regfit.fwd1 <- regsubsets(VioCrime~.,data=dat1, nvmax = 100,
                         method = "backward", really.big = TRUE)

reg.summary1 <- summary(regfit.fwd1)

par(mfrow=c(2,2))
plot(reg.summary1$rsq, xlab = "Number of Variables", ylab ="R-squared")
plot(reg.summary1$adjr2, xlab = "Number of Variables", 
     ylab ="Adj R-squared")
which.max(reg.summary1$adjr2)
maxar3 <- which.max(reg.summary1$adjr2)
points(maxar3,reg.summary1$adjr2[maxar2], col = "red", cex = 2, pch = 20)
plot(reg.summary1$cp, xlab = "Number of Variables", ylab ="Mallows Cp")
which.min(reg.summary1$cp)
mincp1 <- which.min(reg.summary1$cp)
points(mincp1,reg.summary1$cp[mincp1], col = "blue", cex = 2, pch = 20)
plot(reg.summary1$bic, xlab = "Number of Variables", 
     ylab ="Bayesian Info Crit")
which.min(reg.summary1$bic)
minbic1 <- which.min(reg.summary1$bic)
points(minbic1,reg.summary1$bic[minbic1], col = "green", cex = 2, pch = 20)
q4 <- data.frame(maxar3,mincp1,minbic1)
q4
```
The variables for Max Adjusted R squared dropped from 64 to 58 when we ran a
backward stepwise regression compared to forward stepwise regression.
This seems sensitive for selecting ideal or 'good' subset.
Also even though the Mallows Cp and Bayesian's BIC also dropped to 49 
and 14 variables for the subsets compared to 51 and 15 from 
forward stepwise regression respectively. 
It still not the best way to pick up variables to run a regression
on certain subsets. Because, these evaluations just provide a ideal number 
of subsets of number 'N' and don't give any information
on as to which variables are going in these subsets for best accuracy.
Hence, when comparing both the forward and backward stepwise regressions 
they are slightly different.


#### Question 5
```{r}
set.seed(100001)
train <- sample(nrow(dat1),nrow(dat1)/2)
dat.train <- dat1[train,]
dat.test <- dat1[-train,]

reg.train <- lm(VioCrime~ ., data = dat.train)
sum.train <- summary(reg.train)
sum.train$r.squared

train_p5 <- predict(reg.train, dat.train)
test_p5 <- predict(reg.train, dat.test)

RSS_Train5 <- sum((dat.train$VioCrime - train_p5)^2)
RSS_Test5 <- sum((dat.test$VioCrime - test_p5)^2)

MSE_Train5 <- RSS_Train5/(nrow(dat.train))
MSE_Test5 <- RSS_Test5/(nrow(dat.test))

RMSE_Train5 <- sqrt(MSE_Train5)
RMSE_Test5 <- sqrt(MSE_Test5)

tab5 <- matrix(c(RSS_Train5, MSE_Train5, RMSE_Train5,
                RSS_Test5, MSE_Test5, RMSE_Test5), ncol=2, byrow=FALSE)
options(scipen=999)
colnames(tab5) <- c("Training", "Test")
rownames(tab5) <- c("RSS", "MSE", "RMSE")
tab5
```

The R square for the current model on training data is 0.7168268. 
The RMSE for training data is 0.12158294 and for the test data is 0.1416358. 
Given the current numbers, over-fitting seems as an issue in he current model.
The current model fits very well on training data and has an RMSE of 0.12158294,
where as it increases on the test data to 0.1416358. It might not seem lot of
difference initially, but it has to be kept in mind that the data is normalized
and each point has much more weight. The model is over-fitting as well as biased
towards the training data set.

#### Question 6
```{r}
#install.packages("boot")
library(boot)
glm1 <- glm(VioCrime~ ., data = dat1)
glm1_sum <- summary(glm1)
cv.err.5 <- cv.glm(dat1, glm1, K = 5)

MSE.5 <- cv.err.5$delta[2]
RMSE.5 <- MSE.5^0.5
#RMSE.5

cv.err.10 <- cv.glm(dat1, glm1, K = 10)
MSE.10 <- cv.err.10$delta[2]
RMSE.10 <- MSE.10^0.5
#RMSE.10

tab6 <- matrix(c(MSE.5, RMSE.5,
                MSE.10, RMSE.10), ncol=2, byrow=FALSE)
options(scipen=999)
colnames(tab6) <- c("Fold5", "Fold10")
rownames(tab6) <- c("MSE", "RMSE")
tab6
```
#As k-fold creates kind of subsets, I used the whole data instead of train and 
#test data created previously.

Compared to Question 5, the MSE & RMSE are in middle of the values for the train 
and test sets MSE  & RMSE. In Q5 the MSE is around 0.01478241 for train and
0.0200607 for test, where as the values from Question 6 5folds and 10folds
are 0.01848577 and 0.01861804 respectively. 
Similarly in Q5 the RMSE is around 0.12158294 for train and
0.1416358 for test, where as the values from Question 6 5folds and 10folds
are 0.13596240 and 0.13644795 respectively. 
Both the MSE and RMSE values seems to be like sitting at the average levels of
MSE & RMSE for Q5's train and test data.

The reason we don't run LOOCV here is because the data is big and the way LOOCV
performs, it leaves one out and calculates which will become computationally
very expensive and time taking and wouldn't be a optimal approach for such a 
big data set.

#### Question 7
```{r}
#install.packages("ISLR")
library(ISLR)

dat7 <- na.omit(dat1)

#install.packages("glmnet")
library(glmnet)
#
#  Set up our data into y and X
#    X is the design matrix without the intercept
#
y <- dat7$VioCrime
X <- model.matrix(VioCrime~., dat7)[,-1]
dim(X)

grid <- 10^seq(10,-2,length=100)

set.seed(767676)
train <- sample(1:nrow(X), nrow(X)/2)
X.train <- X[train,]
y.train <- y[train]
X.test <- X[-train,]
y.test <- y[-train]

ridge.mod <- glmnet(X.train, y.train, alpha = 0, 
                    lambda = grid, thresh = 1e-12)

ridge.coeff <- matrix(0, nrow = ncol(X), ncol = 100)
ridge.pred <- matrix(0,nrow = length(y.test), ncol = 100)
testerr <- matrix(0, nrow = 100, ncol = 1)

#
for (j in 1:100) {
  ridge.coeff[,j] <- ridge.mod$beta[,j]
  ridge.pred[,j] <- predict(ridge.mod, s = grid[j], 
                            newx = X.test)
  testerr[j] <- mean((ridge.pred[,j] - y.test)^2)
}
#
#  Plot the test MSEs for the 100 models
#
plot(testerr, xlab = "Model Number", 
     ylab = "Test Mean Suqare Error")
best_lambada = grid[which.min(testerr)]
best_lambada
grid[97] #checking the lambda

#
testerr[97]
RMSE.train7 <- testerr[97]^0.5
RMSE.train7

ridge.mod2 <- glmnet(X.train, y.train, alpha = 0, lambada = best_lambda, thresh = 1e-12)
ridge.pred2 = predict(ridge.mod2, s = best_lambada, newx = X.test)
MSE.test7 <- mean((ridge.pred2 - y.test)^2)
MSE.test7

RMSE.test7 <- MSE.test7^0.5
RMSE.test7

```

The MSE for test data is 0.02082086 and RMSE is 0.1442944.

#### Question 8
```{r}
cv.out <- cv.glmnet(X.train, y.train, alpha = 0)
plot(cv.out)

bestlam = cv.out$lambda.min
bestlam


ridge8 <- glmnet(X.train, y.train, alpha = 0, lambda = bestlam)
ridge.pred8 <- predict(ridge8, s = bestlam, newx = X.test)
MSE8 <- mean((ridge.pred8 - y.test)^2)
RMSE8 <- MSE8^0.5
MSE8
RMSE8
```

The MSE for test data is 0.02084079 and RMSE is 0.1443634.

#### Question 9
```{r}
lasso.mod <- glmnet(X.train, y.train, alpha=1, 
                    lambda=bestlam, thresh = 1e-12)
plot(lasso.mod, xvar="lambda", label = TRUE)

cv.out1 <- cv.glmnet(X.train, y.train, alpha = 1)
plot(cv.out1)
bestlam1 <- cv.out1$lambda.min
bestlam1

lasso.pred <- predict(lasso.mod, s=bestlam1, 
                      newx = X.test)
MSE9 <- mean((lasso.pred-y.test)^2)
MSE9
RMSE9 <- MSE9^0.5
RMSE9
```

The MSE for test data using LASSO regression is 0.02108822 
and RMSE is 0.1444718.

#### Question 10

Based on the values from question 5 to question 9, its hard to pin point one 
'fair' value for the MSE and RMSE for given data. But it defiantly helped in 
providing a range to look at.
1. The lowest MSE is 0.01478241 from Q5 training data and highest is from
LASSO regression in Q9 0.02108822. It will ideal to say that the fair value
will be in this range.
2. Similarly the RMSE is lowest in Q5 again of value 0.12158294 and highest 
again is from Q9 LASSO regression at 0.1444718. The ideal value should be 
from within this range with an attempt to be as low as we can go without
overfitting issues.
The fair values are in the above mentioned ranges, where we try to reduce, 
overfitting and bias and come with the lowest possible values.